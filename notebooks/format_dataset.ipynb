{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "import yaml\n",
    "import pygsp\n",
    "import glob\n",
    "import matplotlib\n",
    "\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, Birch, BisectingKMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from pyprojroot import here\n",
    "from tsmoothie import LowessSmoother, ExponentialSmoother\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ROOT_DIR = str(here())\n",
    "insar_path = ROOT_DIR + \"/data/raw/insar/\" \n",
    "\n",
    "pio.templates.default = 'plotly'\n",
    "matplotlib.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE PROCCESSING\n",
    "\n",
    "def interpolate_displacement(df):\n",
    "    interpolated_df = df.set_index('timestamp').resample('6D').ffill()\n",
    "    interpolated_df['displacement'] = (\n",
    "                                       df[['timestamp','displacement']].set_index('timestamp')\n",
    "                                                                       .resample('6D')\n",
    "                                                                       .interpolate(method='linear')\n",
    "                                      )\n",
    "    return interpolated_df\n",
    "\n",
    "\n",
    "def smoothing(frac):\n",
    "    def smoothing_(x):\n",
    "        lowess_smoother = LowessSmoother(smooth_fraction=frac, iterations=1) #0.075 \n",
    "        lowess_smoother.smooth(x)\n",
    "        return lowess_smoother.smooth_data[0]\n",
    "    return smoothing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading original file\n",
    "# Put insar csv file here\n",
    "filename = 'insar_file.csv'\n",
    "\n",
    "df_orig = pd.read_csv(insar_path + filename)\n",
    "#df_orig = pd.read_csv(insar_dir + \"D1/L2B_037_0695_IW2_VV.csv\") # Sensors Paper\n",
    "\n",
    "\n",
    "# Visualization of file density\n",
    "fig = px.density_heatmap(x=df_orig.longitude, y=df_orig.latitude, nbinsx = 100, nbinsy=100, width=600, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT AND FORMAT DATA\n",
    "# Here you can set the region you want to extract from the datafile by setting latitude and longitude boundaries\n",
    "\n",
    "df = df_orig.copy()\n",
    "lat_min, lat_max, lon_min, lon_max = (63.4182, 63.4220, 10.3858, 10.3946) # Sensors Paper\n",
    "\n",
    "df = df[ (df.longitude>lon_min) & (df.longitude<=lon_max) &\n",
    "            (df.latitude>lat_min) & (df.latitude<=lat_max)  ]\n",
    "\n",
    "fig = px.density_heatmap(x=df.longitude, y=df.latitude, nbinsx = 100, nbinsy=100, width=500, height=00)\n",
    "fig.show()\n",
    "\n",
    "# Selection relevant columns\n",
    "date_cols = sorted([col for col in df.columns if \"20\" in col]) #columns named after timestamps\n",
    "keep_cols = date_cols #list with variables to keep from dataframe\n",
    "id_cols = ['pid', 'latitude', 'longitude', 'easting', 'northing', 'mean_velocity']\n",
    "keep_cols.extend(id_cols)\n",
    "df = df[keep_cols]\n",
    "\n",
    "# Formatting from wide to tall dataframe\n",
    "# Uses a single column for timestamp and a column for displacement\n",
    "# Number of rows = number of pixels * number of timestamps\n",
    "df = df.melt(id_vars=id_cols, value_vars=date_cols,\n",
    "                var_name='timestamp', value_name='displacement').sort_values('pid')\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "\n",
    "# Selecing time period: based on gap before 2016.06\n",
    "df = df[df.timestamp>='2016-06-01'].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sort_values(['pid','timestamp'], inplace=True)\n",
    "\n",
    "# CLUSTERING PIXELS (to work with smaller groups at once later)\n",
    "# Simply gives each pixel a new attribute identifying it into a local cluster.\n",
    "average_size = 1000 # Average size of clusters\n",
    "nodes_full = df.drop_duplicates(['pid'])[['pid', 'easting','northing']]\n",
    "nodes_full['cluster'] = KMeans(n_clusters=nodes_full.shape[0]//average_size).fit_predict(nodes_full[['northing','easting']])\n",
    "df = df.merge(nodes_full[['pid','cluster']], how='left', on='pid')\n",
    "\n",
    "print(f'{df.pid.nunique()} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERPOLATE MISSING TIMESTAMPS\n",
    "df = (df.groupby('pid', as_index=False)\n",
    "                .apply(interpolate_displacement)\n",
    "                .reset_index().drop('level_0', axis=1)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY SMOOTHNESS\n",
    "df['smoothed'] = df.groupby('pid',as_index=False).displacement.transform(smoothing(50/df.timestamp.nunique()))\n",
    "# df['smooth60'] = df.groupby('pid',as_index=False).displacement.transform(smoothing(60/df.timestamp.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "output_name = 'output_name.parq'\n",
    "df.to_parquet(ROOT_DIR+f\"/data/interim/{output_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dario-juiScTYW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ec3b9eedf9bbca5af024a8d2af376499af510a5403dc8040ac2fcf9913d231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
